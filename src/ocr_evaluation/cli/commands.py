#!/usr/bin/env python3
"""
CLIå‘½ä»¤å®ç°
"""

import argparse
from pathlib import Path
from typing import Dict, Any, List, Optional
import sys
import json

from ..config import (
    get_config, load_config_from_file, 
    DATA_DIR, IMAGES_DIR, REPORTS_DIR, OUTPUTS_DIR,
    ModelTypes
)
from ..models import create_evaluator, get_supported_models
from ..utils import ReportGenerator, setup_logging, get_logger, create_progress_logger


class BaseCommand:
    """å‘½ä»¤åŸºç±»"""
    
    def __init__(self):
        self.logger = None
    
    def setup_logging(self, args):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # ä»å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®è·å–æ—¥å¿—çº§åˆ«
        log_level = getattr(args, 'log_level', None)
        if log_level:
            config_dict = {'logging': {'level': log_level}}
            setup_logging(config_dict)
        else:
            config = get_config()
            setup_logging(config.config)
        
        self.logger = get_logger(self.__class__.__name__)
    
    def add_common_arguments(self, parser: argparse.ArgumentParser):
        """æ·»åŠ é€šç”¨å‚æ•°"""
        parser.add_argument(
            '--config', '-c',
            type=Path,
            help='é…ç½®æ–‡ä»¶è·¯å¾„ (YAMLæˆ–JSONæ ¼å¼)'
        )
        parser.add_argument(
            '--log-level',
            choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
            help='æ—¥å¿—çº§åˆ«'
        )
        parser.add_argument(
            '--verbose', '-v',
            action='store_true',
            help='è¯¦ç»†è¾“å‡º (ç›¸å½“äº --log-level DEBUG)'
        )
    
    def load_config(self, args):
        """åŠ è½½é…ç½®"""
        if args.verbose:
            args.log_level = 'DEBUG'
        
        if args.config and args.config.exists():
            load_config_from_file(args.config)
            if hasattr(args, 'log_level') and args.log_level:
                config = get_config()
                config.set('logging.level', args.log_level)


class EvaluateCommand(BaseCommand):
    """è¯„ä¼°å‘½ä»¤"""
    
    @staticmethod
    def add_arguments(parser: argparse.ArgumentParser):
        """æ·»åŠ è¯„ä¼°å‘½ä»¤å‚æ•°"""
        parser.add_argument(
            'model',
            choices=get_supported_models(),
            help='è¦ä½¿ç”¨çš„æ¨¡å‹ç±»å‹'
        )
        parser.add_argument(
            '--images-dir', '-i',
            type=Path,
            default=IMAGES_DIR,
            help=f'å›¾ç‰‡ç›®å½•è·¯å¾„ (é»˜è®¤: {IMAGES_DIR})'
        )
        parser.add_argument(
            '--output-dir', '-o',
            type=Path,
            default=REPORTS_DIR,
            help=f'æŠ¥å‘Šè¾“å‡ºç›®å½• (é»˜è®¤: {REPORTS_DIR})'
        )
        parser.add_argument(
            '--model-config',
            type=str,
            help='æ¨¡å‹é…ç½® (JSONæ ¼å¼å­—ç¬¦ä¸²)'
        )
        parser.add_argument(
            '--report-format',
            choices=['markdown', 'json', 'both'],
            default='both',
            help='æŠ¥å‘Šæ ¼å¼ (é»˜è®¤: both)'
        )
        parser.add_argument(
            '--report-name',
            type=str,
            help='è‡ªå®šä¹‰æŠ¥å‘Šæ–‡ä»¶åå‰ç¼€'
        )
    
    def run(self, args):
        """æ‰§è¡Œè¯„ä¼°å‘½ä»¤"""
        self.load_config(args)
        self.setup_logging(args)
        
        self.logger.info("ğŸš€ å¼€å§‹OCRæ¨¡å‹è¯„ä¼°")
        self.logger.info(f"æ¨¡å‹ç±»å‹: {args.model}")
        self.logger.info(f"å›¾ç‰‡ç›®å½•: {args.images_dir}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        
        try:
            # éªŒè¯è¾“å…¥ç›®å½•
            if not args.images_dir.exists():
                self.logger.error(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {args.images_dir}")
                return 1
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            args.output_dir.mkdir(parents=True, exist_ok=True)
            
            # è§£ææ¨¡å‹é…ç½®
            model_config = None
            if args.model_config:
                try:
                    model_config = json.loads(args.model_config)
                except json.JSONDecodeError as e:
                    self.logger.error(f"æ¨¡å‹é…ç½®JSONæ ¼å¼é”™è¯¯: {e}")
                    return 1
            
            # åˆ›å»ºè¯„ä¼°å™¨
            self.logger.info("æ­£åœ¨åˆ›å»ºè¯„ä¼°å™¨...")
            evaluator = create_evaluator(args.model, model_config)
            
            # æ‰§è¡Œè¯„ä¼°
            self.logger.info("æ­£åœ¨æ‰§è¡Œæ•°æ®é›†è¯„ä¼°...")
            summary = evaluator.evaluate_dataset(args.images_dir)
            
            if summary is None:
                self.logger.error("è¯„ä¼°å¤±è´¥")
                return 1
            
            # ç”ŸæˆæŠ¥å‘Š
            self.logger.info("æ­£åœ¨ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
            report_generator = ReportGenerator(args.output_dir)
            
            report_files = []
            
            if args.report_format in ['markdown', 'both']:
                markdown_file = report_generator.save_markdown_report(
                    summary, 
                    args.report_name + '.md' if args.report_name else None
                )
                report_files.append(markdown_file)
            
            if args.report_format in ['json', 'both']:
                json_file = report_generator.save_json_results(
                    summary,
                    args.report_name + '.json' if args.report_name else None
                )
                report_files.append(json_file)
            
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            self._show_summary(summary)
            
            # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
            self.logger.info("\nğŸ“„ ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶:")
            for file_path in report_files:
                self.logger.info(f"  - {file_path}")
            
            self.logger.info("âœ… è¯„ä¼°å®Œæˆ!")
            return 0
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return 1
    
    def _show_summary(self, summary):
        """æ˜¾ç¤ºè¯„ä¼°ç»“æœæ‘˜è¦"""
        self.logger.info("\nğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦:")
        self.logger.info(f"  æ¨¡å‹: {summary.model_name}")
        self.logger.info(f"  æ€»å›¾ç‰‡æ•°: {summary.total_images}")
        self.logger.info(f"  æ€»ä½“å‡†ç¡®ç‡: {summary.overall_accuracy:.4f} ({summary.overall_accuracy*100:.2f}%)")
        self.logger.info(f"  å®Œå…¨åŒ¹é…ç‡: {summary.overall_exact_match_rate:.4f} ({summary.overall_exact_match_rate*100:.2f}%)")
        
        # æ˜¾ç¤ºå„ç›®å½•ç»“æœ
        self.logger.info("\nğŸ“ åˆ†ç›®å½•ç»“æœ:")
        for dir_result in summary.directory_results:
            dir_name = dir_result.directory.name
            self.logger.info(f"  {dir_name}: {dir_result.total_images}å¼ å›¾ç‰‡, "
                           f"å‡†ç¡®ç‡ {dir_result.average_accuracy:.4f} ({dir_result.average_accuracy*100:.2f}%)")


class CompareCommand(BaseCommand):
    """æ¨¡å‹å¯¹æ¯”å‘½ä»¤"""
    
    @staticmethod
    def add_arguments(parser: argparse.ArgumentParser):
        """æ·»åŠ å¯¹æ¯”å‘½ä»¤å‚æ•°"""
        parser.add_argument(
            'models',
            nargs='+',
            choices=get_supported_models(),
            help='è¦å¯¹æ¯”çš„æ¨¡å‹ç±»å‹ (å¯æŒ‡å®šå¤šä¸ª)'
        )
        parser.add_argument(
            '--images-dir', '-i',
            type=Path,
            default=IMAGES_DIR,
            help=f'å›¾ç‰‡ç›®å½•è·¯å¾„ (é»˜è®¤: {IMAGES_DIR})'
        )
        parser.add_argument(
            '--output-dir', '-o',
            type=Path,
            default=REPORTS_DIR,
            help=f'æŠ¥å‘Šè¾“å‡ºç›®å½• (é»˜è®¤: {REPORTS_DIR})'
        )
        parser.add_argument(
            '--comparison-report',
            type=str,
            default='model_comparison',
            help='å¯¹æ¯”æŠ¥å‘Šæ–‡ä»¶åå‰ç¼€ (é»˜è®¤: model_comparison)'
        )
    
    def run(self, args):
        """æ‰§è¡Œæ¨¡å‹å¯¹æ¯”å‘½ä»¤"""
        self.load_config(args)
        self.setup_logging(args)
        
        self.logger.info("ğŸ”„ å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°")
        self.logger.info(f"å¯¹æ¯”æ¨¡å‹: {', '.join(args.models)}")
        
        try:
            # éªŒè¯è¾“å…¥
            if len(args.models) < 2:
                self.logger.error("è‡³å°‘éœ€è¦æŒ‡å®šä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”")
                return 1
            
            if not args.images_dir.exists():
                self.logger.error(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {args.images_dir}")
                return 1
            
            args.output_dir.mkdir(parents=True, exist_ok=True)
            
            # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
            summaries = []
            for model_type in args.models:
                self.logger.info(f"\nğŸ” è¯„ä¼°æ¨¡å‹: {model_type}")
                
                evaluator = create_evaluator(model_type)
                summary = evaluator.evaluate_dataset(args.images_dir)
                
                if summary:
                    summaries.append(summary)
                    self.logger.info(f"  {model_type} å®Œæˆ: å‡†ç¡®ç‡ {summary.overall_accuracy:.4f}")
                else:
                    self.logger.warning(f"  {model_type} è¯„ä¼°å¤±è´¥")
            
            if len(summaries) < 2:
                self.logger.error("è‡³å°‘éœ€è¦ä¸¤ä¸ªæˆåŠŸçš„è¯„ä¼°ç»“æœæ‰èƒ½è¿›è¡Œå¯¹æ¯”")
                return 1
            
            # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
            self.logger.info("\nğŸ“Š ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
            comparison_report = self._generate_comparison_report(summaries)
            
            report_file = args.output_dir / f"{args.comparison_report}.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(comparison_report)
            
            self.logger.info(f"ğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
            
            # æ˜¾ç¤ºå¯¹æ¯”æ‘˜è¦
            self._show_comparison_summary(summaries)
            
            return 0
            
        except Exception as e:
            self.logger.error(f"å¯¹æ¯”è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return 1
    
    def _generate_comparison_report(self, summaries: List) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        from datetime import datetime
        
        report = []
        report.append("# OCRæ¨¡å‹å¯¹æ¯”è¯„ä¼°æŠ¥å‘Š")
        report.append("")
        report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**å¯¹æ¯”æ¨¡å‹**: {', '.join([s.model_name for s in summaries])}")
        report.append("")
        
        # æ€»ä½“å¯¹æ¯”è¡¨æ ¼
        report.append("## ğŸ“Š æ€»ä½“æ€§èƒ½å¯¹æ¯”")
        report.append("")
        report.append("| æ¨¡å‹ | æ€»å›¾ç‰‡æ•° | æ€»ä½“å‡†ç¡®ç‡ | å®Œå…¨åŒ¹é…ç‡ | å¹³å‡å¤„ç†æ—¶é—´ |")
        report.append("|------|----------|------------|------------|--------------|")
        
        for summary in summaries:
            processing_time = summary.technical_details.get('average_processing_time', 0)
            report.append(f"| {summary.model_name} | {summary.total_images} | "
                         f"{summary.overall_accuracy:.4f} ({summary.overall_accuracy*100:.2f}%) | "
                         f"{summary.overall_exact_match_rate:.4f} ({summary.overall_exact_match_rate*100:.2f}%) | "
                         f"{processing_time:.3f}s |")
        
        report.append("")
        
        # åˆ†ç›®å½•å¯¹æ¯”
        report.append("## ğŸ“ åˆ†ç›®å½•æ€§èƒ½å¯¹æ¯”")
        report.append("")
        
        # è·å–æ‰€æœ‰ç›®å½•
        all_directories = set()
        for summary in summaries:
            for dir_result in summary.directory_results:
                all_directories.add(dir_result.directory.name)
        
        for directory in sorted(all_directories):
            report.append(f"### {directory}")
            report.append("")
            report.append("| æ¨¡å‹ | å›¾ç‰‡æ•°é‡ | å‡†ç¡®ç‡ | å®Œå…¨åŒ¹é…ç‡ |")
            report.append("|------|----------|--------|------------|")
            
            for summary in summaries:
                dir_result = next(
                    (dr for dr in summary.directory_results if dr.directory.name == directory),
                    None
                )
                if dir_result:
                    report.append(f"| {summary.model_name} | {dir_result.total_images} | "
                                 f"{dir_result.average_accuracy:.4f} ({dir_result.average_accuracy*100:.2f}%) | "
                                 f"{dir_result.exact_match_rate:.4f} ({dir_result.exact_match_rate*100:.2f}%) |")
                else:
                    report.append(f"| {summary.model_name} | - | - | - |")
            
            report.append("")
        
        # ç»“è®ºå’Œå»ºè®®
        report.append("## ğŸ’¡ ç»“è®ºå’Œå»ºè®®")
        report.append("")
        
        # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„æ¨¡å‹
        best_model = max(summaries, key=lambda s: s.overall_accuracy)
        report.append(f"### æœ€ä½³æ•´ä½“æ€§èƒ½: {best_model.model_name}")
        report.append(f"- å‡†ç¡®ç‡: {best_model.overall_accuracy:.4f} ({best_model.overall_accuracy*100:.2f}%)")
        report.append(f"- å®Œå…¨åŒ¹é…ç‡: {best_model.overall_exact_match_rate:.4f} ({best_model.overall_exact_match_rate*100:.2f}%)")
        report.append("")
        
        # å„æ¨¡å‹ä¼˜åŠ¿åˆ†æ
        report.append("### æ¨¡å‹ç‰¹æ€§åˆ†æ")
        report.append("")
        for summary in summaries:
            model_type = summary.technical_details.get('model_type', 'unknown')
            if model_type == 'paddleocr':
                report.append(f"**{summary.model_name}** (ä¸“ä¸šOCRæ¨¡å‹):")
                report.append("- âœ… ä¸“é—¨é’ˆå¯¹æ–‡å­—è¯†åˆ«ä¼˜åŒ–")
                report.append("- âœ… å¤„ç†é€Ÿåº¦å¿«")
                report.append("- âŒ åŠŸèƒ½ç›¸å¯¹å•ä¸€")
            elif model_type == 'qwen_vl':
                report.append(f"**{summary.model_name}** (å¤šæ¨¡æ€LLM):")
                report.append("- âœ… å…·å¤‡å›¾åƒç†è§£èƒ½åŠ›")
                report.append("- âœ… å¯å¤„ç†å¤æ‚åœºæ™¯")
                report.append("- âŒ æ¨ç†é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢")
            report.append("")
        
        return "\n".join(report)
    
    def _show_comparison_summary(self, summaries: List):
        """æ˜¾ç¤ºå¯¹æ¯”æ‘˜è¦"""
        self.logger.info("\nğŸ“Š æ¨¡å‹å¯¹æ¯”æ‘˜è¦:")
        
        # æŒ‰å‡†ç¡®ç‡æ’åº
        sorted_summaries = sorted(summaries, key=lambda s: s.overall_accuracy, reverse=True)
        
        for i, summary in enumerate(sorted_summaries, 1):
            self.logger.info(f"  {i}. {summary.model_name}: "
                           f"å‡†ç¡®ç‡ {summary.overall_accuracy:.4f} ({summary.overall_accuracy*100:.2f}%), "
                           f"åŒ¹é…ç‡ {summary.overall_exact_match_rate:.4f} ({summary.overall_exact_match_rate*100:.2f}%)")


class ConfigCommand(BaseCommand):
    """é…ç½®ç®¡ç†å‘½ä»¤"""
    
    @staticmethod
    def add_arguments(parser: argparse.ArgumentParser):
        """æ·»åŠ é…ç½®å‘½ä»¤å‚æ•°"""
        subparsers = parser.add_subparsers(dest='config_action', help='é…ç½®æ“ä½œ')
        
        # æ˜¾ç¤ºé…ç½®
        show_parser = subparsers.add_parser('show', help='æ˜¾ç¤ºå½“å‰é…ç½®')
        show_parser.add_argument(
            '--key', '-k',
            type=str,
            help='æ˜¾ç¤ºç‰¹å®šé…ç½®é¡¹ (æ”¯æŒç‚¹åˆ†å‰²è·¯å¾„ï¼Œå¦‚ models.paddleocr.lang)'
        )
        
        # è®¾ç½®é…ç½®
        set_parser = subparsers.add_parser('set', help='è®¾ç½®é…ç½®é¡¹')
        set_parser.add_argument('key', help='é…ç½®é¡¹é”®å (æ”¯æŒç‚¹åˆ†å‰²è·¯å¾„)')
        set_parser.add_argument('value', help='é…ç½®é¡¹å€¼')
        
        # ç”Ÿæˆé»˜è®¤é…ç½®æ–‡ä»¶
        generate_parser = subparsers.add_parser('generate', help='ç”Ÿæˆé»˜è®¤é…ç½®æ–‡ä»¶')
        generate_parser.add_argument(
            '--output', '-o',
            type=Path,
            default='config.yaml',
            help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.yaml)'
        )
        generate_parser.add_argument(
            '--format',
            choices=['yaml', 'json'],
            default='yaml',
            help='é…ç½®æ–‡ä»¶æ ¼å¼ (é»˜è®¤: yaml)'
        )
    
    def run(self, args):
        """æ‰§è¡Œé…ç½®å‘½ä»¤"""
        self.load_config(args)
        self.setup_logging(args)
        
        if args.config_action == 'show':
            return self._show_config(args)
        elif args.config_action == 'set':
            return self._set_config(args)
        elif args.config_action == 'generate':
            return self._generate_config(args)
        else:
            self.logger.error("æœªæŒ‡å®šé…ç½®æ“ä½œï¼Œä½¿ç”¨ --help æŸ¥çœ‹å¯ç”¨æ“ä½œ")
            return 1
    
    def _show_config(self, args) -> int:
        """æ˜¾ç¤ºé…ç½®"""
        config = get_config()
        
        if args.key:
            # æ˜¾ç¤ºç‰¹å®šé…ç½®é¡¹
            value = config.get(args.key)
            if value is not None:
                self.logger.info(f"{args.key}: {value}")
            else:
                self.logger.error(f"é…ç½®é¡¹ '{args.key}' ä¸å­˜åœ¨")
                return 1
        else:
            # æ˜¾ç¤ºå®Œæ•´é…ç½®
            import yaml
            config_yaml = yaml.dump(config.config, default_flow_style=False, 
                                   allow_unicode=True, indent=2)
            print(config_yaml)
        
        return 0
    
    def _set_config(self, args) -> int:
        """è®¾ç½®é…ç½®é¡¹"""
        config = get_config()
        
        # å°è¯•è§£æå€¼çš„ç±»å‹
        value = args.value
        if value.lower() in ('true', 'false'):
            value = value.lower() == 'true'
        elif value.isdigit():
            value = int(value)
        elif value.replace('.', '').isdigit():
            value = float(value)
        
        config.set(args.key, value)
        self.logger.info(f"é…ç½®é¡¹ '{args.key}' å·²è®¾ç½®ä¸º: {value}")
        
        return 0
    
    def _generate_config(self, args) -> int:
        """ç”Ÿæˆé»˜è®¤é…ç½®æ–‡ä»¶"""
        try:
            config = get_config()
            config.save_config(args.output)
            self.logger.info(f"é»˜è®¤é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {args.output}")
            return 0
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return 1